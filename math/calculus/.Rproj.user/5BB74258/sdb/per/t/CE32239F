{
    "contents" : "---\ntitle: \"Notes Chapter 1 - Introduction to Data\"\noutput: pdf_document\n---\n\n```{r, echo=FALSE}\nsetwd(\"C:/Chris/stats/oi_stats\")\n#install.packages(\"openintro\")\nlibrary(ggplot2)\nlibrary(openintro)\nlibrary(scales)\n```\n\nAnalyses are often a four step process:\n\n1. Identify a question or problem.\n2. Collect relevant data on the topic.\n3. Analyze the data.\n4. Form a conclusion.\n\nThe subject of statistics tries to make steps 2-4 objective, rigorous, and\nefficient.\n\n# 1.1 Case Study\n\n*Summary Statistic*: A single number summarizing a large amount of data.\n\n\n# 1.2 Data Basics\n\n## 1.2.1 Observations, Variables, and Data Matrices\n\nA data matrix is a common way to display data. In it a row corresponds to a case\n(or unit of observation) and each column represents a variable. An example for \na data matrix with 21 variables and 50 cases is the email50 dataset as loaded \nhere:\n\n```{r}\nemail50 <- read.table(\"data/email50.txt\", header = TRUE, sep = \"\\t\")\nhead(email50)\n```\n\n\n## 1.2.2 Types of Variables\n\nExample data set county:\n\n```{r}\ncounty <- read.table(\"data/county.txt\", header = T, sep = \"\\t\")\ncounty_w_sb <- read.table(\"data/county_w_sm_ban.txt\", header = T, sep = \"\\t\")\ncounty_w_sb_slim <- county_w_sb[,c(1,2,4,5,51,33,26,27,31,32,54)]\nhead(county_w_sb_slim)\nhead(county)\n```\n\nVariables can be:\n\n* Numerical: Variable is a number and it makes sense to perform calculations \nbased on it. E.g. fed_spending or pop2010 in the county dataset. As a counter \nexample, zip-codes are not numerical variables because calculations with zip-\ncodes do not make sense. Numerical variables can be split into:  \n    + Continuous: Variable can take values in a certain range. E.g. fed_spending.\n    + Discrete: Variable can take only values of certain steps. E.g. pop2010, a \n    population of 25.34 does not make sense.\n* Categorical: Categorical variables take values which categorize observations. \nE.g. the state or smoking_ban column in the country dataset. Categorical \nvariables can be:\n    + Ordinal: These variables have a (natural) ordering.\n    + Regular: These variables have no ordering.\n\n## 1.2.3 Relationship Between Variables\n\n```{r, echo=FALSE}\ng <- ggplot(county, aes(poverty/100, fed_spend))\ng <- g + geom_point(color = \"black\", size = 2, alpha = 1/2)\ng <- g + scale_x_continuous(name = \"Percent of Units in Multi-Unit Structures\",\n                            limit = c(0, 0.6), labels = percent)\ng <- g + scale_y_continuous(name = \"Percent of Homeownership\",\n                            limit = c(0, 35), labels = comma)\n\n\ng <- g + geom_segment(aes(x=county[county$name==\"Owsley County\",]$poverty/100,\n                          y=0, \n                          xend = county[county$name==\"Owsley County\",]$poverty/100,\n                          yend=county[county$name==\"Owsley County\",]$fed_spend,\n                          color=\"red\"),\n                      linetype = 2, show_guide = FALSE)\ng <- g + geom_segment(aes(x=0,\n                          y=county[county$name==\"Owsley County\",]$fed_spend,\n                          xend=county[county$name==\"Owsley County\",]$poverty/100,\n                          yend=county[county$name==\"Owsley County\",]$fed_spend,\n                          color=\"red\"),\n                      linetype = 2, show_guide = FALSE)\ng <- g + geom_point(x = county[county$name==\"Owsley County\",]$poverty/100,\n                    y = county[county$name==\"Owsley County\",]$fed_spend,\n                    color = \"red\", size = 4, alpha = 0.75, shape = 1)\ng\n```\n\nVariables can be associated (also called dependent), which means they have some\nkind of relationship with each other, or independent. Furthermore the association\n(or dependency) can be positive (see plot above) or negative (see plot below).\n\n```{r, echo=FALSE}\ng <- ggplot(county, aes(multiunit/100, homeownership/100))\ng <- g + geom_point(color = \"black\", size = 2, alpha = 1/2)\ng <- g + scale_x_continuous(name = \"Percent of Units in Multi-Unit Structures\",\n                            limit = c(0, 1), labels = percent)\ng <- g + scale_y_continuous(name = \"Percent of Homeownership\",\n                            limit = c(0, 1), labels = percent)\ng\n```\n\n\n# 1.3 Overview of Data Collection Principles\n\nAfter identifying the problem upon which research is supposed to be conducted,\nit has to be considered how data are collected so that they are reliable and \nuseful for research.\n\n## 1.3.1 Populations and Samples\n\nExample research questions:\n\n1. What is the average mercury content in swordfish in the Atlantic Ocean?\n2. Over the last 5 years, what is the average time to complete a degree for \nDuke undergraduate students?\n3. Does a new drug reduce the number of deaths in patients with severe heart\ndisease?\n\nResearch question often refer to a target population. These populations are in\nmany cases so large that it is too expensive to collect data for every case.\nInstead a sample is taken, representing an (often small) subset of the \npopulation.\n\n## 1.3.2 Anecdotal Evidence\n\nAnecdotal Evidence is evidence that is based on too few cases (often low single\ndigit number) and which are often not representative of the population. We are\nmore likely to remember unusual or personal cases, which is why these\noften cloud our judgement. Instead we should collect sufficient, representative \ndata and use it to answer a research question in a sound way.\n\nExample answers to the questions above based on anecdotal evidence:\n\n1. A man on the news got mercury poisoning from eating swordfish, so the average\nmercury concentration in swordfish must be dangerously high.\n2. I met two students who took more than 7 years to graduate from Duke, so it\nmust take longer to graduate at Duke than at many other colleges.\n3. My friend's dad had a heart attack and died after they gave him a new heart \ndisease drug, so the drug must not work.\n\n## 1.3.3 Sampling from a Population\n\nUnless impossible, samples should always be choosen __randomly__ from a \npopulation. When cases are selected manually, there is a chance of introducing\n__bias__ into the sample. A basic way of sampling randomly is called \n__simple random sample__, which is similar to a raffle lottery. Here each case\nhas the same chance of being included and there is no connection between the\ncases in the sample.\n\nAnother type of bias is the __non-response bias__, which is very common in \nsurveys. It happens when a certain part of the population is more likely to\nrespond to the survey. Especially when response rates are low, one must be\ncautious.\n\nA third common bias is the one of having a __convenience sample__. Here cases\nthat are easily accessible are more likely to be included in the sample. This\ncould happen when a survey is taken only is a small part of the area for which\nit is supposed to be.\n\n\n## 1.3.4 Explanatory and Response Variables\n\nAn __explanatory variable__ is one that might affect the __response variable__.\nIn datasets with many variables there are often many of these relationships. \nOther terms are __independent variable__ and __dependent variable__ for the \nexplanatory (does not depend on the response variable) and the response variable\nrespectively.\n\nE.g. in the county dataset one might say that the poverty rate is the\nexplanatory variable for federal spending.\n\nCaution: Just because two variables are associated there is not necessarily a \n__causal relationship__ between them. __Association does not imply causation.__\n\nAlso even if there is association between two variables, the direction might not\nbe clear. One example for this is the homeownership rate and the percentage of\nmulti-unit structures in the county dataset.\n\n## 1.3.5 Introducing Observational Studies and Experiments\n\nObservational Studies and Experiments are the two primary types of data\ncollection. \n\nIn __observational studies__ data is collected in a way that does not interfere\nwith how the data arises. E.g. medical records or already existing company\nrecords. Generally observational studies can only provide evidence of a \nnaturally occuring association, while they can not show causal connection.\n\nIn order to prove causality an __experiment__ needs to be conducted. Here\nexplanatory and response variables are defined upfront. Afterwards the sample\nis split into groups. If this is done randomly we speak of a \n__randomized experiment__. Each group is then assigned a different input for the\nexplanatory variables. E.g. when researching a medical treatment, one group\ncould get the treatment while the other group gets a placebo.\n\n\n# 1.4 Observational Studies and Sampling Strategies\n\n## 1.4.1 Observational Studies\n\nData in observational studies are collected by monitoring what occurs. This \nallows the researcher to show association, but it is dangerous, and therefore\nnot recommended, to use them to infer causality. For this an experiment is\nusually required.\n\n__Confounding variables__, which are correlated to both the explanatory and the\nresponse variable, play a great role in observational studies. E.g. sunscreen \nusage and skin cancer are associated variables, however one does not cause the\nother. A confounding variable in this case is sun exposure.\n\nConfounding variables are a reason why observational studies can not prove\ncausality well. In order to do so one must exhaust the search for confounding \nvariables, which is very hard to do.\n\nConfounding variables are also called __lurking variables__, __confounder__, or\n__confounding factor__.\n\nObservational studies come in two forms:\n\n* __Prospective__: Individuals/cases are identified and data is collected as\nevents unfold. E.g. observing a group of similar individuals over time to see\nif certain aspects of their behaviour influence cancer risk.\n* __Retrospective__: Data is collected after events have taken place. E.g.\nmedical records.\n\n\n## 1.4.2 Three Sampling Methods\n\n__Simple random sampling__ is the most intuitive form of random sampling. All\ncases are randomly drawn from the total population. This is comparable to a \nraffle lottery. In general simple random sampling is characterized by each case\nhaving the same probability of being included in the sample and by there being\nno relationship between the cases selected and not selected to be included in \nthe sample.\n\nIn __stratified sampling__ the population is divided into groups of cases, \ncalled _strata__, which are similar to each other with respect to the outcome of\ninterest. Then from each stratum a number of cases are choosen by simple random \nsampling. This technique is very useful if the cases within each stratum are\nvery similar to each other. However the statistical methods described here need\nto be extended to be used on data collected using stratified sampling.\n\nIn __cluster sampling__ the population is divided into groups of cases called\n__clusters__. Afterwards a fixed number of cases are sampled from some clusters\nvia simple random sampling. It is not required to sample from all clusters.\nCluster sampling is most helpful when there is a lot of case-by-case variability\nwithing the clusters, but the clusters themselves are very similar to each other.\nIt is often a more economical sampling method than the other two. However the \nmethods in this book need to be extended before the can be used on data \ncollected via cluster sampling.\n\n\n# 1.5 Experiments\n\nStudies in which researchers assign treatments to cases are called \n__experiments__. When this assignment is done in a randomized way, the study is\na __randomized experiment__, which are fundamentally important when trying to\nshow a causal connection between variables.\n\n\n## 1.5.1 Principles of Experiment Design\n\n__Controlling__: After assigning treatments to cases, the researcher must do\nhis best to control any other factors that might influence the outcome of the\nobserved variable. It is essential to track all these other factors in variables.\n\n__Randomization__: By assigning cases to the treatment and control group in a \nrandomized way, the researcher not only avoids bias, he also accounts for \nvariables that can not be controlled by making sure they affect both groups \nequally.\n\n__Replication__: The more cases are included in a study, the more accurately the\neffect between explanatory and response variable can be estimated. In addition\nto replicating cases within a study (by simply having many of them), a whole\nstudy can be replicated by another researcher to varify findings.\n\n__Blocking__: When the researcher suspects that uncontrollable variables affect\nthe outcome of the study, he may first group cases into __blocks__. Afterwards\nhe randomizes for each block seperately. The methods in this book need to first\nbe extended to analyze data from experiments that use blocking.\n\n\n## 1.5.2 Reducing Bias in Human Experiments\n\nWhile the researcher must always do his best to aviod bias in a study it often\narises unintentionally. This is especially common in studies involving humans in\nany way. One way to avoid unintentional bias is to design a __blind__ study, in\nwhich the patient is unaware about which treatment he receives. To aid this a \n__placebo__ can be used. In some cases slight but real improvements happen for\ncases that receive the placebo (e.g. because of emotional effects). This is \ncalled the __placebo effect__. Because bias can also be introduced by the \nperson administrating the study he should also be unaware of which case belongs\nto which group. These studies are then __double-blind__.\n\n# 1.6 Examining Numerical Data\n\n## 1.6.1 Scatterplots for Paired Data\n\n```{r, echo=FALSE}\nemail50 <- read.table(\"data/email50.txt\", sep = \"\\t\", header = TRUE)\n\ng <- ggplot(data = email50)\ng <- g + geom_point(aes(x = num_char, y = line_breaks), alpha = 1/2, \n                    color = \"steelblue\", size = 4)\ng <- g + scale_x_continuous(name = \"Number of Characters (in thousands)\")\ng <- g + scale_y_continuous(name = \"Number of Lines\")\ng\n```\n\nThis __scatterplot__ visualizes the relationship between two variables from the\nemail50 dataset. Scatterplots are helpful when illustrating association between\nvariables. However they can not show causality. Also they might miss \nrelationships. E.g. in the email50 dataset, many of the more verbose mails are\nHTML, which increases the number of characters without increasing the amount of\ncontent.\n\nScatterplots can be enhanced in many ways. E.g. the plots below adds three more\nvariables, by varying color, size, and facet by the spam, exclaim_mess, and \nformat variables.\n\n```{r, echo=FALSE}\nemail50$spam <- as.factor(email50$spam)\np <- ggplot(data = email50)\np <- p + geom_point(aes(x = num_char, y = line_breaks, color = spam, \n                    size = exclaim_mess))\np <- p + scale_color_manual(values = c(\"green\", \"red\"))\np <- p + scale_x_continuous(name = \"Number of Characters (in thousands)\")\np <- p + scale_y_continuous(name = \"Number of Lines\")\np <- p + facet_grid(. ~ format)\np\n```\n\n\n## 1.6.2 Dot Plots and the Mean\n\n```{r, echo=FALSE}\ng <- ggplot(data = email50)\ng <- g + geom_dotplot(aes(x = num_char))\n#g <- g + scale_y_continuous(name = \"\", breaks = NULL)\ng <- g + geom_segment(aes(x = mean(num_char), xend = mean(num_char), y = 0,\n                      yend = 0.25, color = \"red\"), show_guide = F)\ng\n```\n\nA __dot plot__ is essentially a one variable scatterplot. It is useful to show\nthe __mean__ (average) of the data. The mean of the sample data is called\n__sample mean__, usually denoted by $\\bar{x}$ (x_bar), and computed as follows:\n\n$$\\bar{x} = \\frac{\\sum (x_1 + x_2 + ... + x_n)}{n}$$\n\nHere $\\mathbf{n}$ is the number of observations (exlcuding NAs) and \n$\\mathbf{x_i}$ are the values observed for each variable. The equivalent \ncalculation in R:\n\n```{r}\nsum(email50$num_char) / length(email50$num_char)\nmean(email50$num_char)\n```\n\nThe mean of the populations, the __population mean__ is represented by the greek\nletter mu: $\\mathbf{\\mu}$. A subscript can be added to show which variable it\nrefers to: $\\mathbf{\\mu_x}$.\n\nIt is often important to calculate the __weighted mean__, especially when using\ndata that is already aggregated. Instead of dividing by the number of \nobservations the weighted mean divides by the sum of the weights $w_i$ that each\nvariable has:\n\n$$\\bar{x} = \\frac{\\sum (x_1 * w_1 + x_2 * w_2 + ... + x_n * w_n)}\n    {\\sum (w_1 + w_2 + ... + w_n)}$$\n\nE.g. for the county dataset one could compute the mean income per county as the\nnormal mean. However for the average income per person in the US, a weighted\nmean needs to be used.\n\n```{r}\ncounty <- read.table(\"data/county.txt\", header = T, sep = \"\\t\")\nmean(county$income)\nweighted.mean(county$income, county$pop2010)\nsum(as.double(county$income) * as.double(county$pop2010)) / sum(county$pop2010)\n```\n\n\n## 1.6.3 Histograms and Shape\n\nA __histogram__ is similar to a dotplot, however is displays the frequency as \nthe height of the bar on the y axis (instead of the number of dots) and groups\nobservations into __bins__.\n\n```{r}\nemail50 <- read.table(\"data/email50.txt\", sep = \"\\t\", header = TRUE)\ng <- ggplot(email50)\ng <- g + geom_histogram(aes(x = num_char), binwidth = 5, color = \"black\",\n                        fill = \"steelblue\")\ng <- g + xlab(\"Number of characters (in thousands)\") + ylab(\"Frequency\")\ng\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1449386157796.000,
    "dirty" : false,
    "encoding" : "ISO8859-1",
    "folds" : "",
    "hash" : "3016463096",
    "id" : "CE32239F",
    "lastKnownWriteTime" : 1449386458,
    "path" : "C:/Chris/stats/books/oi_stats/notes_chapter_1_intro.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}